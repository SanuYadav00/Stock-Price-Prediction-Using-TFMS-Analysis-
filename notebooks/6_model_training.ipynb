{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d98524c-0b01-490c-b050-f95051bcd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # TFMS Stock Prediction - Model Training (Final Fixed Version)\n",
    "\n",
    "# %%\n",
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Create results directories\n",
    "os.makedirs(\"../data/results/models\", exist_ok=True)\n",
    "os.makedirs(\"../data/results/metrics\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1122b83f-c2cc-4a25-b85b-5704e37fcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 2: Data Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def load_prepare_data(ticker):\n",
    "    df = pd.read_csv(f\"../data/processed/integrated/{ticker}_integrated.csv\", parse_dates=['Date'])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    technical_features = ['RSI_14', 'SMA_20', 'Volume']\n",
    "    fundamental_features = ['debt_to_equity', 'sentiment_3day_ma']\n",
    "    lookback = 60  # 60 timesteps\n",
    "    n_features = len(technical_features)  # Should be 3\n",
    "    \n",
    "    # Calculate maximum complete sequences\n",
    "    n_samples = (len(df) - lookback + 1)\n",
    "    \n",
    "    # Create properly shaped technical data\n",
    "    X_tech = np.zeros((n_samples, lookback, n_features))\n",
    "    y = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        X_tech[i] = df[technical_features].iloc[i:i+lookback].values\n",
    "        y[i] = df['target_5day_return'].iloc[i+lookback-1]\n",
    "    \n",
    "    # Get aligned fundamental data\n",
    "    X_fund = df[fundamental_features].iloc[lookback-1:lookback-1+n_samples].values\n",
    "    \n",
    "    # Split data (80/20)\n",
    "    split_idx = int(0.8 * n_samples)\n",
    "    \n",
    "    # Scale technical features\n",
    "    scaler = StandardScaler()\n",
    "    X_tech_scaled = scaler.fit_transform(\n",
    "        X_tech.reshape(-1, n_features)\n",
    "    ).reshape(X_tech.shape)\n",
    "    \n",
    "    return {\n",
    "        'X_tech_train': X_tech_scaled[:split_idx],\n",
    "        'X_tech_test': X_tech_scaled[split_idx:],\n",
    "        'X_fund_train': X_fund[:split_idx],\n",
    "        'X_fund_test': X_fund[split_idx:],\n",
    "        'y_train': y[:split_idx],\n",
    "        'y_test': y[split_idx:],\n",
    "        'features': {\n",
    "            'technical': technical_features,\n",
    "            'fundamental': fundamental_features\n",
    "        },\n",
    "        'scalers': {\n",
    "            'tech_scaler': scaler\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae7d05c-65fa-4fad-b92a-557b57451d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 3: Fixed Keras Regressor Wrapper\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "# Updated KerasRegressorWrapper\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class KerasRegressorWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model_builder, input_shape, epochs=30):\n",
    "        self.model_builder = model_builder\n",
    "        self.input_shape = input_shape\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self._temp_model_file = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Extract technical features (first lookback*n_features columns)\n",
    "        n_features = self.input_shape[-1]\n",
    "        lookback = self.input_shape[0]\n",
    "        tech_features = lookback * n_features\n",
    "        X_tech = X[:, :tech_features]\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        X_reshaped = X_tech.reshape(-1, lookback, n_features)\n",
    "        \n",
    "        # Build and train model\n",
    "        self._model = self.model_builder(self.input_shape)\n",
    "        self._model.compile(optimizer='adam', loss='mse')\n",
    "        self._model.fit(X_reshaped, y, epochs=self.epochs, verbose=0)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Extract technical features\n",
    "        n_features = self.input_shape[-1]\n",
    "        lookback = self.input_shape[0]\n",
    "        tech_features = lookback * n_features\n",
    "        X_tech = X[:, :tech_features]\n",
    "        \n",
    "        # Reshape and predict\n",
    "        X_reshaped = X_tech.reshape(-1, lookback, n_features)\n",
    "        return self._model.predict(X_reshaped).flatten()\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        # Custom serialization to handle Keras model\n",
    "        state = self.__dict__.copy()\n",
    "        if self._model is not None:\n",
    "            # Save model to temporary file\n",
    "            fd, path = tempfile.mkstemp(suffix='.h5')\n",
    "            try:\n",
    "                os.close(fd)\n",
    "                save_model(self._model, path)\n",
    "                with open(path, 'rb') as f:\n",
    "                    state['_model_data'] = f.read()\n",
    "            finally:\n",
    "                os.remove(path)\n",
    "            del state['_model']\n",
    "        return state\n",
    "    \n",
    "    def __setstate__(self, state):\n",
    "        # Custom deserialization\n",
    "        if '_model_data' in state:\n",
    "            # Restore model from temporary file data\n",
    "            fd, path = tempfile.mkstemp(suffix='.h5')\n",
    "            try:\n",
    "                os.close(fd)\n",
    "                with open(path, 'wb') as f:\n",
    "                    f.write(state['_model_data'])\n",
    "                state['_model'] = load_model(path)\n",
    "                del state['_model_data']\n",
    "            finally:\n",
    "                os.remove(path)\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_ensemble(models, X_train, y_train, ticker):\n",
    "    lookback = 60\n",
    "    n_features = 3\n",
    "    \n",
    "    # Technical features should already be properly shaped\n",
    "    X_tech = X_train['technical']\n",
    "    n_samples = X_tech.shape[0]\n",
    "    \n",
    "    # Flatten technical features for stacking\n",
    "    X_tech_flat = X_tech.reshape(n_samples, -1)  # Shape: (n_samples, lookback*n_features)\n",
    "    \n",
    "    # Align fundamental features\n",
    "    X_fund = X_train['fundamental'][:n_samples]  # Ensure same number of samples\n",
    "    y_aligned = y_train[:n_samples]\n",
    "    \n",
    "    # Combine features - now only using the correct number of technical features\n",
    "    tech_feature_cols = lookback * n_features\n",
    "    X_combined = np.hstack([X_tech_flat[:, :tech_feature_cols], X_fund])\n",
    "    \n",
    "    print(f\"Final feature matrix shape: {X_combined.shape}\")\n",
    "    print(f\"Target shape: {y_aligned.shape}\")\n",
    "    \n",
    "    # Create Keras wrapper with persistent model builder\n",
    "    keras_reg = KerasRegressorWrapper(\n",
    "        model_builder=build_technical_model,\n",
    "        input_shape=(lookback, n_features),\n",
    "        epochs=30\n",
    "    )\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = StackingRegressor(\n",
    "        estimators=[\n",
    "            ('technical', keras_reg),\n",
    "            ('fundamental', clone(models['fundamental']))\n",
    "        ],\n",
    "        final_estimator=RandomForestRegressor(n_estimators=50),\n",
    "        cv=3,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    ensemble.fit(X_combined, y_aligned)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b09eac-0795-4da0-ac6c-1136305f1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 4: Model Training Functions\n",
    "def build_technical_model(input_shape):\n",
    "    \"\"\"Model builder function that returns a fresh compiled model\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(64,\n",
    "             activation='tanh',\n",
    "             recurrent_activation='sigmoid',\n",
    "             kernel_initializer='glorot_uniform',\n",
    "             recurrent_initializer='orthogonal',\n",
    "             kernel_constraint=MaxNorm(3),\n",
    "             recurrent_constraint=MaxNorm(3)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_technical_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train LSTM model with proper initialization and gradient control\"\"\"\n",
    "    # Normalize targets\n",
    "    y_mean, y_std = y_train.mean(), y_train.std()\n",
    "    y_train_norm = (y_train - y_mean) / y_std\n",
    "    y_test_norm = (y_test - y_mean) / y_std\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = build_technical_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, clipvalue=1.0),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Train with callbacks\n",
    "    history = model.fit(\n",
    "        X_train, y_train_norm,\n",
    "        validation_data=(X_test, y_test_norm),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store normalization parameters\n",
    "    model.y_mean = y_mean\n",
    "    model.y_std = y_std\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_fundamental_model(X_train, y_train):\n",
    "    \"\"\"XGBoost model for fundamentals\"\"\"\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_technical_features(X_tech, lookback=60, n_features=3):\n",
    "    \"\"\"Ensure data can be reshaped by trimming excess elements\"\"\"\n",
    "    elements_per_sample = lookback * n_features  # 180\n",
    "    total_elements = (X_tech.size // elements_per_sample) * elements_per_sample\n",
    "    \n",
    "    # Trim to largest whole number of samples\n",
    "    X_trimmed = X_tech[:total_elements]\n",
    "    \n",
    "    # Reshape\n",
    "    n_samples = total_elements // elements_per_sample\n",
    "    X_reshaped = X_trimmed.reshape(n_samples, lookback, n_features)\n",
    "    X_flat = X_reshaped.reshape(n_samples, -1)  # Flatten for stacking\n",
    "    \n",
    "    print(f\"Trimmed {X_tech.size - total_elements} elements\")\n",
    "    print(f\"Final shape: {X_reshaped.shape}\")\n",
    "    return X_reshaped, X_flat\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f370d0-f46a-47b3-b04f-400687bbe08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training pipeline for XOM...\n",
      "\n",
      "Loading and preparing data...\n",
      "Data loaded successfully with 171 training samples\n",
      "\n",
      "X_tech_train shape: (171, 60, 3)\n",
      "X_fund_train shape: (171, 2)\n",
      "y_train shape: (171,)\n",
      "X_tech_train samples: 171\n",
      "X_fund_train samples: 171\n",
      "y_train samples: 171\n",
      "Training technical model (LSTM)...\n",
      "Epoch 1/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 200ms/step - loss: 0.7458 - mae: 0.6645 - val_loss: 0.3903 - val_mae: 0.4936 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.5566 - mae: 0.5925 - val_loss: 0.3423 - val_mae: 0.4990 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6037 - mae: 0.6259 - val_loss: 0.3271 - val_mae: 0.5042 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.5191 - mae: 0.5892 - val_loss: 0.3250 - val_mae: 0.4998 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.5349 - mae: 0.5974 - val_loss: 0.3130 - val_mae: 0.4876 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.5472 - mae: 0.5989 - val_loss: 0.3164 - val_mae: 0.4864 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4649 - mae: 0.5533 - val_loss: 0.3116 - val_mae: 0.4811 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.5167 - mae: 0.5863 - val_loss: 0.3090 - val_mae: 0.4792 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4714 - mae: 0.5396 - val_loss: 0.3061 - val_mae: 0.4781 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4674 - mae: 0.5573 - val_loss: 0.3063 - val_mae: 0.4775 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.4809 - mae: 0.5521 - val_loss: 0.3116 - val_mae: 0.4790 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.4197 - mae: 0.5152 - val_loss: 0.3184 - val_mae: 0.4785 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4750 - mae: 0.5541 - val_loss: 0.3200 - val_mae: 0.4779 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4445 - mae: 0.5410 - val_loss: 0.3142 - val_mae: 0.4758 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4443 - mae: 0.5240 - val_loss: 0.3145 - val_mae: 0.4757 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.4484 - mae: 0.5414 - val_loss: 0.3142 - val_mae: 0.4766 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.4800 - mae: 0.5722 - val_loss: 0.3134 - val_mae: 0.4766 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.4154 - mae: 0.5223 - val_loss: 0.3142 - val_mae: 0.4763 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4559 - mae: 0.5512 - val_loss: 0.3137 - val_mae: 0.4762 - learning_rate: 5.0000e-04\n",
      "Technical model trained and saved\n",
      "\n",
      "Training fundamental model (XGBoost)...\n",
      "Fundamental model trained and saved\n",
      "\n",
      "Training ensemble model...\n",
      "Final feature matrix shape: (171, 182)\n",
      "Target shape: (171,)\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 487ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000027D29FE04A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 534ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000027D29FE04A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model trained and saved\n",
      "\n",
      "Evaluating models...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "=== Final Metrics ===\n",
      "\n",
      "technical:\n",
      "RMSE: 0.3532\n",
      "Direction Accuracy: 62.79%\n",
      "\n",
      "fundamental:\n",
      "RMSE: 0.0386\n",
      "Direction Accuracy: 51.16%\n",
      "\n",
      "ensemble:\n",
      "RMSE: 0.0265\n",
      "Direction Accuracy: 62.79%\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Main Execution (Complete)\n",
    "\n",
    "# %%\n",
    "# Configuration\n",
    "ticker = 'XOM'\n",
    "print(f\"Starting training pipeline for {ticker}...\\n\")\n",
    "\n",
    "# 1. Load and prepare data\n",
    "print(\"Loading and preparing data...\")\n",
    "data = load_prepare_data(ticker)\n",
    "print(f\"Data loaded successfully with {len(data['X_tech_train'])} training samples\\n\")\n",
    "\n",
    "data = load_prepare_data(ticker)\n",
    "print(\"X_tech_train shape:\", data['X_tech_train'].shape)\n",
    "print(\"X_fund_train shape:\", data['X_fund_train'].shape)\n",
    "print(\"y_train shape:\", data['y_train'].shape)\n",
    "\n",
    "print(\"X_tech_train samples:\", data['X_tech_train'].shape[0])\n",
    "print(\"X_fund_train samples:\", data['X_fund_train'].shape[0])\n",
    "print(\"y_train samples:\", data['y_train'].shape[0])\n",
    "\n",
    "# # Debug prints\n",
    "# print(f\"Raw technical data shape: {X_train['technical'].shape}\")\n",
    "# print(f\"Fundamental data shape: {X_train['fundamental'].shape}\")\n",
    "# print(f\"Target shape: {y_train.shape}\")\n",
    "\n",
    "# # Sample calculation\n",
    "# n_samples = X_train['technical'].shape[0] // (60 * 3)\n",
    "# print(f\"Can create {n_samples} complete sequences\")\n",
    "\n",
    "# 2. Train models\n",
    "print(\"Training technical model (LSTM)...\")\n",
    "tech_model = train_technical_model(\n",
    "    data['X_tech_train'],\n",
    "    data['y_train'],      # Using the correct key\n",
    "    data['X_tech_test'],\n",
    "    data['y_test']\n",
    ")\n",
    "tech_model.save(f\"../data/results/models/{ticker}_lstm.keras\")\n",
    "print(\"Technical model trained and saved\\n\")\n",
    "\n",
    "print(\"Training fundamental model (XGBoost)...\")\n",
    "fund_model = train_fundamental_model(\n",
    "    data['X_fund_train'],\n",
    "    data['y_train']\n",
    ")\n",
    "joblib.dump(fund_model, f\"../data/results/models/{ticker}_xgb.pkl\")\n",
    "print(\"Fundamental model trained and saved\\n\")\n",
    "\n",
    "\n",
    "print(\"Training ensemble model...\")\n",
    "ensemble = train_ensemble(\n",
    "    {'technical': tech_model, 'fundamental': fund_model},\n",
    "    {'technical': data['X_tech_train'], 'fundamental': data['X_fund_train']},\n",
    "    data['y_train'],\n",
    "    ticker\n",
    ")\n",
    "\n",
    "# Save ensemble model\n",
    "try:\n",
    "    joblib.dump(ensemble, f\"../data/results/models/{ticker}_ensemble.pkl\")\n",
    "    print(\"Ensemble model trained and saved\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving ensemble model: {str(e)}\")\n",
    "    # Alternative save method if primary fails\n",
    "    with open(f\"../data/results/models/{ticker}_ensemble_fallback.pkl\", 'wb') as f:\n",
    "        pickle.dump(ensemble, f)\n",
    "    print(\"Used fallback method to save ensemble model\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Evaluate models\n",
    "print(\"Evaluating models...\")\n",
    "results = {}\n",
    "\n",
    "# Technical evaluation\n",
    "tech_X_test = data['X_tech_test']\n",
    "results['technical'] = {\n",
    "    'rmse': np.sqrt(mean_squared_error(\n",
    "        data['y_test'],\n",
    "        tech_model.predict(tech_X_test)\n",
    "    )),\n",
    "    'direction_accuracy': accuracy_score(\n",
    "        np.sign(data['y_test']),\n",
    "        np.sign(tech_model.predict(tech_X_test))\n",
    "    )\n",
    "}\n",
    "\n",
    "# Fundamental evaluation\n",
    "results['fundamental'] = {\n",
    "    'rmse': np.sqrt(mean_squared_error(\n",
    "        data['y_test'],\n",
    "        fund_model.predict(data['X_fund_test'])\n",
    "    )),\n",
    "    'direction_accuracy': accuracy_score(\n",
    "        np.sign(data['y_test']),\n",
    "        np.sign(fund_model.predict(data['X_fund_test']))\n",
    "    )\n",
    "}\n",
    "\n",
    "# Ensemble evaluation\n",
    "X_tech_reshaped = tech_X_test.reshape(tech_X_test.shape[0], -1)\n",
    "X_combined_test = np.hstack([X_tech_reshaped, data['X_fund_test']])\n",
    "results['ensemble'] = {\n",
    "    'rmse': np.sqrt(mean_squared_error(\n",
    "        data['y_test'],\n",
    "        ensemble.predict(X_combined_test)\n",
    "    )),\n",
    "    'direction_accuracy': accuracy_score(\n",
    "        np.sign(data['y_test']),\n",
    "        np.sign(ensemble.predict(X_combined_test))\n",
    "    )\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "with open(f\"../data/results/metrics/{ticker}_metrics.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# 4. Display results\n",
    "print(\"\\n=== Final Metrics ===\")\n",
    "for model_type, metrics in results.items():\n",
    "    print(f\"\\n{model_type}:\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"Direction Accuracy: {metrics['direction_accuracy']:.2%}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3a2bc9-03e7-46dc-89c5-70bfeadb1f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting feature importance...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Feature importance\n",
    "# 5. Feature importance\n",
    "print(\"\\nPlotting feature importance...\")\n",
    "\n",
    "# For StackingRegressor, we need to get feature importances differently\n",
    "if hasattr(ensemble.final_estimator_, 'feature_importances_'):\n",
    "    # Get the names of the base models\n",
    "    model_names = [name for name, _ in ensemble.estimators]\n",
    "    \n",
    "    # Create DataFrame with model importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': model_names,\n",
    "        'importance': ensemble.final_estimator_.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title('Model Importance in Ensemble')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../data/results/metrics/{ticker}_feature_importance.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Final estimator doesn't have feature importances. Using coefficients instead.\")\n",
    "    \n",
    "    if hasattr(ensemble.final_estimator_, 'coef_'):\n",
    "        model_names = [name for name, _ in ensemble.estimators]\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': model_names,\n",
    "            'importance': np.abs(ensemble.final_estimator_.coef_)\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "        plt.title('Model Coefficients in Ensemble')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../data/results/metrics/{ticker}_feature_importance.png\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Cannot compute feature importance - final estimator doesn't support it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc66c64-bf40-465b-b0cb-123c46e13b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating predictions...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "Technical Model Predictions:\n",
      "Predictions - Min: -1.0804, Max: 0.4684, Mean: 0.0110\n",
      "Actual - Min: -0.0564, Max: 0.0543, Mean: 0.0022\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "Ensemble Model Predictions:\n",
      "Predictions - Min: -0.0572, Max: 0.0524, Mean: 0.0128\n",
      "Actual - Min: -0.0564, Max: 0.0543, Mean: 0.0022\n",
      "\n",
      "Training pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# 6. Prediction validation\n",
    "print(\"\\nValidating predictions...\")\n",
    "def check_predictions(model, X, y_true, model_type):\n",
    "    preds = model.predict(X)\n",
    "    print(f\"\\n{model_type} Model Predictions:\")\n",
    "    print(f\"Predictions - Min: {preds.min():.4f}, Max: {preds.max():.4f}, Mean: {preds.mean():.4f}\")\n",
    "    print(f\"Actual - Min: {y_true.min():.4f}, Max: {y_true.max():.4f}, Mean: {y_true.mean():.4f}\")\n",
    "    assert not np.isnan(preds).any()\n",
    "\n",
    "check_predictions(tech_model, data['X_tech_test'][:100], data['y_test'][:100], \"Technical\")\n",
    "check_predictions(ensemble, X_combined_test[:100], data['y_test'][:100], \"Ensemble\")\n",
    "\n",
    "print(\"\\nTraining pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
